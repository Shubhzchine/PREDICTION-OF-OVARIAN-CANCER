# -*- coding: utf-8 -*-
"""ResNet 50.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15sZ1IyUIxOWZnyCNOBArvJNy5VYpitLP
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import matplotlib.pyplot as plt
import os
import math
import shutil
import glob

# count the number of images in the respective classes 0 - Brain Tumor and 1 - Healthy
ROOT_DIR = "/content/drive/MyDrive/original folder"
number_of_images = {}

for dir in os.listdir(ROOT_DIR):
    number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)))
    number_of_images.items()

number_of_images = {}

for dir in os.listdir(ROOT_DIR):
    number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)) )
number_of_images.items()

import os
import numpy as np
import math
import shutil

def dataFolder(p, split):
    """
    This function creates a new folder (train/val) and copies images
    from the original directory without deleting them.
    """

    # Ensure dataset directory exists
    ROOT_DIR = "/content/drive/MyDrive/original folder"
    if not os.path.exists(ROOT_DIR):
        raise FileNotFoundError(f"Dataset directory '{ROOT_DIR}' not found!")

    # Create the target directory if it doesn't exist
    os.makedirs(p, exist_ok=True)

    # Count total images per category
    number_of_images = {dir: len(os.listdir(os.path.join(ROOT_DIR, dir))) for dir in os.listdir(ROOT_DIR)}

    for dir in os.listdir(ROOT_DIR):
        category_path = os.path.join(p, dir)
        os.makedirs(category_path, exist_ok=True)  # Prevent FileExistsError

        img_list = os.listdir(os.path.join(ROOT_DIR, dir))
        sample_size = min(len(img_list), max(0, math.floor(split * number_of_images[dir]) - 5))

        if sample_size > 0:
            selected_images = np.random.choice(img_list, size=sample_size, replace=False)

            for img in selected_images:
                src_path = os.path.join(ROOT_DIR, dir, img)
                dest_path = os.path.join(category_path, img)

                if not os.path.exists(dest_path):  # Prevent overwriting
                    shutil.copy(src_path, dest_path)  # âœ… Copy instead of move

    print(f"âœ… Images successfully copied to '{p}' without deletion.")

# Example Usage
#dataFolder("val", 0.15)  # 15% images copied for validation

dataFolder("train", 0.7)

dataFolder("val",0.2)

dataFolder("test", 0.1)

number_of_images = {}

for dir in os.listdir(ROOT_DIR):
    number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)) )
number_of_images.items()

import os
import numpy as np
import math
import shutil

ROOT_DIR = "/content/drive/MyDrive/Original Dataset"

#  Ensure dataset exists
if not os.path.exists(ROOT_DIR):
    raise FileNotFoundError(f"Dataset directory '{ROOT_DIR}' not found!")

#  Create 'train' directory if it doesn't exist
train_path = "./train"
os.makedirs(train_path, exist_ok=True)

#  Get number of images per class
number_of_images = {dir: len(os.listdir(os.path.join(ROOT_DIR, dir))) for dir in os.listdir(ROOT_DIR)}

for dir in os.listdir(ROOT_DIR):
    class_dir = os.path.join(train_path, dir)
    os.makedirs(class_dir, exist_ok=True)  #  Ensure class subfolder exists

    img_list = os.listdir(os.path.join(ROOT_DIR, dir))
    sample_size = max(0, math.floor(0.7 * number_of_images[dir]) - 5)  #  Ensure valid sample size

    if sample_size > 0:
        selected_images = np.random.choice(a=img_list, size=sample_size, replace=False)

        for img in selected_images:
            src_path = os.path.join(ROOT_DIR, dir, img)
            dest_path = os.path.join(class_dir, img)

            shutil.copy(src_path, dest_path)  #  Use copy to preserve originals

print("âœ… Train Folder setup completed successfully without deleting original images!")

"""Build model"""

!pip uninstall keras -y

!pip install --upgrade --force-reinstall tensorflow

from sklearn.metrics import classification_report, confusion_matrix, accuracy_score
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Input, Dense, Flatten, Dropout, GlobalAveragePooling2D
from tensorflow.keras.models import Model
from tensorflow.keras.optimizers import Adam



# 1. Load base model
base_model = ResNet50(weights='imagenet', include_top=False, input_tensor=Input(shape=(224, 224, 3)))

# 2. Freeze all layers initially (optional but good practice)
for layer in base_model.layers:
    layer.trainable = False

# 3. Add custom top layers
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.5)(x)
predictions = Dense(1, activation='sigmoid')(x)
model = Model(inputs=base_model.input, outputs=predictions)

# 4. Compile the model (initial training with frozen layers)
model.compile(optimizer=Adam(learning_rate=1e-4), loss='binary_crossentropy', metrics=['accuracy'])

# 5. Train with frozen base first (optional but helps stabilize)
# model.fit(train_generator, validation_data=val_generator, epochs=5)

# 6. Unfreeze last N layers of base model
for layer in base_model.layers[-30:]:
    layer.trainable = True

# 7. Recompile with a lower learning rate for fine-tuning
model.compile(optimizer=Adam(learning_rate=1e-5), loss='binary_crossentropy', metrics=['accuracy'])

"""preparing our data using datagenretor"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator
def preprocessingImages1(path):
    """
    input  : Path
    output : Pre processed images
    """

    image_data = ImageDataGenerator(zoom_range=0.2, shear_range=0.1, rescale=1/255, horizontal_flip=True)
    image = image_data.flow_from_directory(directory=path, target_size=(224,224), batch_size=32, class_mode='binary')

    return image

path = "/content/train"
train_data = preprocessingImages1(path)

def preprocessingImages2(path):
    """
    input  : Path
    output : Pre processed images
    """
    image_data = ImageDataGenerator(rescale= 1/255)
    image = image_data.flow_from_directory(directory = path, target_size = (224,224), batch_size = 32, class_mode = 'binary')

    return image

path = "/content/test"
test_data = preprocessingImages2(path)

path = "/content/val"
val_data = preprocessingImages2(path)

"""remedies"""

import os
for category in os.listdir(path):
    print(f"{category}: {len(os.listdir(os.path.join(path, category)))} images")

"""1)setup for train generator"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Define the image directory and size
train_dir = "/content/train"  # adjust path as needed
IMG_SIZE = 224
BATCH_SIZE = 32

# Create ImageDataGenerator instance
train_datagen = ImageDataGenerator(rescale=1./255)

# Flow training images from directory
train_generator = train_datagen.flow_from_directory(
    train_dir,
    target_size=(IMG_SIZE, IMG_SIZE),
    batch_size=BATCH_SIZE,
    class_mode='binary'  # or 'categorical' if using one-hot labels
)

"""2)setup for val generator"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Set validation directory
val_dir = "/content/val"  # Make sure path is correct

# Create ImageDataGenerator for validation (usually only rescaling)
val_datagen = ImageDataGenerator(rescale=1./255)

# Flow validation images from directory
val_generator = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False  # Important if you're evaluating predictions later
)

"""3)balance datasset"""

from sklearn.utils import class_weight
import numpy as np

y_train = train_generator.classes
class_weights = class_weight.compute_class_weight(class_weight='balanced',
                                                  classes=np.unique(y_train),
                                                  y=y_train)
class_weights_dict = dict(enumerate(class_weights))
model.fit(train_generator, validation_data=val_generator, class_weight=class_weights_dict)

"""4)Verify the label mapping"""

print(train_generator.class_indices)

# Early stopping and model check point

from keras.callbacks import ModelCheckpoint, EarlyStopping

# early stopping
es = EarlyStopping(monitor="val_accuracy", min_delta=0.01, patience=25, verbose=1, mode='auto')

# model check point
mc = ModelCheckpoint(monitor="val_accuracy", filepath="./bestmodel.h5", verbose=1, save_best_only=True, mode='auto')#.h5 file save as keras

cd = [es,mc]

"""train model"""

hs = model.fit(train_data,  #  No need for 'generator='
               steps_per_epoch=8,
               epochs=50,
               verbose=1,
               validation_data=val_data,
               validation_steps=4,
               callbacks=cd)  # Include early stopping & model checkpoint

"""save the history file"""

import os
import pickle

#  Create the folder if it doesn't exist
os.makedirs('/content/drive/MyDrive/model_history', exist_ok=True)

#  Save the history object correctly
with open('/content/drive/MyDrive/model_history/history.pkl', 'wb') as f:
    pickle.dump(hs.history, f)

print("âœ… Training history saved successfully!")

"""confusion matrix"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

val_datagen = ImageDataGenerator(rescale=1./255)

val_generator = val_datagen.flow_from_directory(
    '/content/val',
    target_size=(224, 224),  # or 35,35 if your model input is that
    batch_size=32,
    class_mode='binary',
    shuffle=False  # Important! For consistent label ordering
)

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd  # for table display

# Step 1: Get model predictions on validation data
y_pred_prob = model.predict(val_generator)

# Assuming binary classification, convert probs to class labels:
y_pred = (y_pred_prob > 0.5).astype(int).reshape(-1)  # flatten if needed

# Step 2: Get true labels
y_true = val_generator.classes

# If y_true is one-hot encoded (unlikely with val_generator.classes), convert:
# y_true = np.argmax(y_true, axis=1)

# Step 3: Classification report (added here)
report_dict = classification_report(y_true, y_pred, output_dict=True)
report_df = pd.DataFrame(report_dict).transpose()
report_df = report_df.round(2)
print("Classification Report:\n", report_df)

# Step 4: Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Step 5: Plot confusion matrix
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

# Model Graphical Interpretation

h = hs.history
h.keys()

import matplotlib.pyplot as plt

acc = hs.history['accuracy']
val_acc = hs.history.get('val_accuracy')
loss = hs.history['loss']
val_loss = hs.history.get('val_loss')

epochs = range(1, len(acc) + 1)

plt.figure(figsize=(16, 12))

# 1. Training Accuracy vs Training Loss
plt.subplot(2, 2, 1)
plt.plot(epochs, acc, 'bo-', label='Training Accuracy')
plt.plot(epochs, loss, 'ro-', label='Training Loss')
plt.title('Training Accuracy vs Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Value')
plt.legend()
plt.grid(True)

# 2. Training Accuracy vs Validation Accuracy
plt.subplot(2, 2, 2)
if val_acc:
    plt.plot(epochs, acc, 'bo-', label='Training Accuracy')
    plt.plot(epochs, val_acc, 'go-', label='Validation Accuracy')
    plt.title('Training Accuracy vs Validation Accuracy')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.grid(True)
else:
    plt.text(0.5, 0.5, 'No validation accuracy data', ha='center', va='center')
    plt.axis('off')

# 3. Training Loss vs Validation Loss
plt.subplot(2, 2, 3)
if val_loss:
    plt.plot(epochs, loss, 'ro-', label='Training Loss')
    plt.plot(epochs, val_loss, 'mo-', label='Validation Loss')
    plt.title('Training Loss vs Validation Loss')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.legend()
    plt.grid(True)
else:
    plt.text(0.5, 0.5, 'No validation loss data', ha='center', va='center')
    plt.axis('off')

# 4. Validation Accuracy vs Validation Loss
plt.subplot(2, 2, 4)
if val_acc and val_loss:
    plt.plot(val_acc, val_loss, 'cs-')
    plt.scatter(val_acc, val_loss, color='cyan')
    plt.title('Validation Accuracy vs Validation Loss')
    plt.xlabel('Validation Accuracy')
    plt.ylabel('Validation Loss')
    plt.grid(True)
else:
    plt.text(0.5, 0.5, 'No validation data available', ha='center', va='center')
    plt.axis('off')

plt.tight_layout()
plt.show()

# Model Accuracy
from keras.models import load_model

model = load_model("/content/bestmodel.h5")

"""plot graph"""

import pickle
import matplotlib.pyplot as plt

# Step 1: Load the saved history
history_path = "/content/drive/MyDrive/model_history/history.pkl"  #  Update if needed

with open(history_path, 'rb') as f:
    history = pickle.load(f)

# Step 2: Extract data
acc = history['accuracy']
val_acc = history['val_accuracy']
loss = history['loss']
val_loss = history['val_loss']
epochs = range(1, len(acc) + 1)

# Helper function to add data points on the graph
def annotate_values(x, y):
    for i, j in zip(x, y):
        plt.text(i, j, f'{j:.2f}', fontsize=8, ha='center', va='bottom')

# ================================
# 1. Training Accuracy vs Training Loss
# ================================
plt.figure(figsize=(8, 5))
plt.plot(epochs, acc, 'bo-', label='Training Accuracy')
plt.plot(epochs, loss, 'ro-', label='Training Loss')
annotate_values(epochs, acc)
annotate_values(epochs, loss)
plt.title('Training Accuracy vs Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.show()

# ================================
# 2. Training Accuracy vs Validation Accuracy
# ================================
plt.figure(figsize=(8, 5))
plt.plot(epochs, acc, 'bo-', label='Training Accuracy')
plt.plot(epochs, val_acc, 'go-', label='Validation Accuracy')
annotate_values(epochs, acc)
annotate_values(epochs, val_acc)
plt.title('Training Accuracy vs Validation Accuracy')
plt.xlabel('Epochs')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)
plt.show()

# ================================
# 3. Training Loss vs Validation Loss
# ================================
plt.figure(figsize=(8, 5))
plt.plot(epochs, loss, 'ro-', label='Training Loss')
plt.plot(epochs, val_loss, 'mo-', label='Validation Loss')
annotate_values(epochs, loss)
annotate_values(epochs, val_loss)
plt.title('Training Loss vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()

# ================================
# 4. Validation Accuracy vs Validation Loss
# ================================
plt.figure(figsize=(8, 5))
plt.plot(epochs, val_acc, 'go-', label='Validation Accuracy')
plt.plot(epochs, val_loss, 'mo-', label='Validation Loss')
annotate_values(epochs, val_acc)
annotate_values(epochs, val_loss)
plt.title('Validation Accuracy vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.show()

"""Model Accuracy"""

acc = model.evaluate(test_data)[1]
print(f"The accuracy of our model is {acc * 100:.2f} %")

"""load history next time plot graph"""

import pickle

# Path to the saved history
history_path = "/content/drive/MyDrive/model_history/history.pkl"

#  Load the history object
with open(history_path, 'rb') as f:
    loaded_history = pickle.load(f)

print("âœ… Training history loaded successfully!")

"""load model"""

from tensorflow.keras.models import load_model
model = load_model('/content/drive/MyDrive/models/best_model.h5')

"""load the file"""

#  Paths to your saved data
train_dir = "/content/drive/MyDrive/ovarian_data/train"
val_dir   = "/content/drive/MyDrive/ovarian_data/val"
test_dir  = "/content/drive/MyDrive/ovarian_data/test"

#  Use ImageDataGenerator to load again
from tensorflow.keras.preprocessing.image import ImageDataGenerator

train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=10, zoom_range=0.2, horizontal_flip=True)
val_datagen = ImageDataGenerator(rescale=1./255)

train_data = train_datagen.flow_from_directory(train_dir, target_size=(224, 224), batch_size=32, class_mode='binary')
val_data   = val_datagen.flow_from_directory(val_dir, target_size=(224, 224), batch_size=32, class_mode='binary')
test_data  = val_datagen.flow_from_directory(test_dir, target_size=(224, 224), batch_size=32, class_mode='binary', shuffle=False)

"""calculate accuracy"""

#  Evaluate training accuracy
train_loss, train_acc = model.evaluate(train_data, verbose=0)
print(f"âœ… Training Accuracy: {train_acc * 100:.2f}%")

#  Evaluate validation accuracy
val_loss, val_acc = model.evaluate(val_data, verbose=0)
print(f"ðŸ§ª Validation Accuracy: {val_acc * 100:.2f}%")

#  Evaluate test accuracy
test_loss, test_acc = model.evaluate(test_data, verbose=0)
print(f"ðŸ§¾ Test Accuracy: {test_acc * 100:.2f}%")

"""testing"""

from tensorflow.keras.preprocessing.image import load_img, img_to_array

from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np
import matplotlib.pyplot as plt

path = "/content/test/Non Cancerous/Copy of 10_341325_aug_12.jpg"

# Load and preprocess image
img = load_img(path, target_size=(224, 224))
input_arr = img_to_array(img) / 255.0

# Show image
plt.imshow(input_arr)
plt.title("Input Image")
plt.axis("off")
plt.show()

# Expand dimensions for batch
input_arr = np.expand_dims(input_arr, axis=0)

# Predict
pred = model.predict(input_arr)
pred_class = (pred > 0.5).astype("int32")

# Print result
print(f"Predicted Class: {pred_class[0][0]} (Raw Prob: {pred[0][0]:.4f})")

if pred_class[0][0] == 0:
    print("ðŸ§¬ This image is **Cancerous**")
else:
    print(" This image is **Non Cancerous**")

train_data.class_indices

"""save model on google drive"""

import os
os.makedirs('/content/drive/MyDrive/modelsres', exist_ok=True)

import shutil
shutil.copy('/content/bestmodel.h5', '/content/drive/MyDrive/modelsres/bestmodel.h5')