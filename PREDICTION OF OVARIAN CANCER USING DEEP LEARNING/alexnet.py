# -*- coding: utf-8 -*-
"""AlexNet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pY6uIYwM4rrSggrSSz-RegIHd0drVMtL
"""

import warnings
warnings.filterwarnings('ignore')

import numpy as np
import matplotlib.pyplot as plt
import os
import math
import shutil
import glob

"""for delete"""

import shutil
import os

# Replace with the path of the folder you want to delete
folder_path = '/content/train'

# Check if folder exists before deleting
if os.path.exists(folder_path):
    shutil.rmtree(folder_path)
    print(f"Folder '{folder_path}' has been deleted.")
else:
    print(f"Folder '{folder_path}' does not exist.")

# count the number of images in the respective classes 0 - Brain Tumor and 1 - Healthy
ROOT_DIR = "/content/drive/MyDrive/original folder"
number_of_images = {}

for dir in os.listdir(ROOT_DIR):
    number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)))
    number_of_images.items()

number_of_images = {}

for dir in os.listdir(ROOT_DIR):
    number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)) )
number_of_images.items()

import os
import numpy as np
import math
import shutil

def dataFolder(p, split):
    """
    This function creates a new folder (train/val) and copies images
    from the original directory without deleting them.
    """

    # Ensure dataset directory exists
    ROOT_DIR = "/content/drive/MyDrive/original folder"
    if not os.path.exists(ROOT_DIR):
        raise FileNotFoundError(f"Dataset directory '{ROOT_DIR}' not found!")

    # Create the target directory if it doesn't exist
    os.makedirs(p, exist_ok=True)

    # Count total images per category
    number_of_images = {dir: len(os.listdir(os.path.join(ROOT_DIR, dir))) for dir in os.listdir(ROOT_DIR)}

    for dir in os.listdir(ROOT_DIR):
        category_path = os.path.join(p, dir)
        os.makedirs(category_path, exist_ok=True)  # Prevent FileExistsError

        img_list = os.listdir(os.path.join(ROOT_DIR, dir))
        sample_size = min(len(img_list), max(0, math.floor(split * number_of_images[dir]) - 5))

        if sample_size > 0:
            selected_images = np.random.choice(img_list, size=sample_size, replace=False)

            for img in selected_images:
                src_path = os.path.join(ROOT_DIR, dir, img)
                dest_path = os.path.join(category_path, img)

                if not os.path.exists(dest_path):  # Prevent overwriting
                    shutil.copy(src_path, dest_path)  # ✅ Copy instead of move

    print(f" Images successfully copied to '{p}' without deletion.")

# Example Usage
#dataFolder("val", 0.15)  # 15% images copied for validation

dataFolder("train", 0.7)

dataFolder("val",0.2)

dataFolder("test", 0.1)

number_of_images = {}

for dir in os.listdir(ROOT_DIR):
    number_of_images[dir] = len(os.listdir(os.path.join(ROOT_DIR, dir)) )
number_of_images.items()

import os
import numpy as np
import math
import shutil

ROOT_DIR = "/content/drive/MyDrive/original folder"

#  Ensure dataset exists
if not os.path.exists(ROOT_DIR):
    raise FileNotFoundError(f"Dataset directory '{ROOT_DIR}' not found!")

#  Create 'train' directory if it doesn't exist
train_path = "./train"
os.makedirs(train_path, exist_ok=True)

#  Get number of images per class
number_of_images = {dir: len(os.listdir(os.path.join(ROOT_DIR, dir))) for dir in os.listdir(ROOT_DIR)}

for dir in os.listdir(ROOT_DIR):
    class_dir = os.path.join(train_path, dir)
    os.makedirs(class_dir, exist_ok=True)  # ✅ Ensure class subfolder exists

    img_list = os.listdir(os.path.join(ROOT_DIR, dir))
    sample_size = max(0, math.floor(0.7 * number_of_images[dir]) - 5)  # ✅ Ensure valid sample size

    if sample_size > 0:
        selected_images = np.random.choice(a=img_list, size=sample_size, replace=False)

        for img in selected_images:
            src_path = os.path.join(ROOT_DIR, dir, img)
            dest_path = os.path.join(class_dir, img)

            shutil.copy(src_path, dest_path)  # ✅ Use copy to preserve originals

print(" Train Folder setup completed successfully without deleting original images!")

"""Build model"""

#!pip uninstall keras -y

!pip install --upgrade --force-reinstall tensorflow

import os
import numpy as np
import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout
from tensorflow.keras.optimizers import Adam
from sklearn.utils import class_weight
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay

train_dir = "/content/train"
val_dir = "/content/val"

train_datagen = ImageDataGenerator(rescale=1./255, rotation_range=10, zoom_range=0.2, horizontal_flip=True)
val_datagen = ImageDataGenerator(rescale=1./255)

train_data = train_datagen.flow_from_directory(
    train_dir,
  target_size=(150, 150),
    batch_size=16,
    class_mode='binary'
)

val_data = val_datagen.flow_from_directory(
    val_dir,
   target_size=(150, 150),
    batch_size=16,
    class_mode='binary'
)

print("Class indices:", train_data.class_indices)

from tensorflow.keras.layers import GlobalAveragePooling2D

model = Sequential([
    Conv2D(96, (11, 11), strides=(4, 4), activation='relu', input_shape=(150, 150, 3)),  # or (128, 128, 3)
    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

    Conv2D(256, (5, 5), padding='same', activation='relu'),
    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

    Conv2D(384, (3, 3), padding='same', activation='relu'),
    Conv2D(384, (3, 3), padding='same', activation='relu'),
    Conv2D(256, (3, 3), padding='same', activation='relu'),
    MaxPooling2D(pool_size=(3, 3), strides=(2, 2)),

    GlobalAveragePooling2D(),  #  replaces Flatten()
    Dense(512, activation='relu'),
    Dropout(0.5),
    Dense(1, activation='sigmoid')  # For binary classification
])

model.compile(optimizer=Adam(learning_rate=0.0001),
              loss='binary_crossentropy',
              metrics=['accuracy'])

model.summary()

class_weights = class_weight.compute_class_weight(
    class_weight='balanced',
    classes=np.unique(train_data.classes),
    y=train_data.classes
)
class_weights_dict = dict(enumerate(class_weights))
print("Class Weights:", class_weights_dict)

from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint

#  Set path to save the best model
checkpoint_path = "best_model.h5"

#  Callback to save the best model based on validation loss
model_checkpoint = ModelCheckpoint(
    filepath=checkpoint_path,
    monitor='val_loss',           # Or use 'val_accuracy'
    save_best_only=True,
    save_weights_only=False,
    verbose=1
)

#  EarlyStopping callback
early_stop = EarlyStopping(
    monitor='val_loss',
    patience=15,
    restore_best_weights=True,
    verbose=1
)

history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=40,
    class_weight=class_weights_dict,
    callbacks=[early_stop, model_checkpoint],
    verbose=1
)

# Early stopping and model check point

from keras.callbacks import ModelCheckpoint, EarlyStopping

# early stopping
es = EarlyStopping(monitor="val_accuracy", min_delta=0.01, patience=40, verbose=1, mode='auto')

# model check point
mc = ModelCheckpoint(monitor="val_accuracy", filepath="./bestmodel.h5", verbose=1, save_best_only=True, mode='auto')#.h5 file save as keras

cd = [es,mc]

"""train model"""

hs = model.fit(train_data,  #  No need for 'generator='
               steps_per_epoch=8,
               epochs=60,
               verbose=1,
               validation_data=val_data,
               validation_steps=4,
               callbacks=cd)  # Include early stopping & model checkpoint

"""confusion matrix"""

from tensorflow.keras.preprocessing.image import ImageDataGenerator

val_datagen = ImageDataGenerator(rescale=1./255)

val_generator = val_datagen.flow_from_directory(
    '/content/val',
    target_size=(224, 224),  # or 35,35 if your model input is that
    batch_size=32,
    class_mode='binary',
    shuffle=False  # Important! For consistent label ordering
)

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np

# Step 1: Get model predictions on validation data
y_pred_prob = model.predict(val_generator)
y_pred = (y_pred_prob > 0.5).astype(int)  # or use 0.6 or 0.7 based on tuning

# Step 2: Get true labels
y_true = val_generator.classes

# Step 3: Compute confusion matrix
cm = confusion_matrix(y_true, y_pred)

# Step 4: Plot it
plt.figure(figsize=(6,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

from sklearn.metrics import classification_report
import pandas as pd

# Get predictions on validation data
val_data.reset()  # Make sure to reset generator
y_pred_probs = model.predict(val_data)
y_pred = (y_pred_probs > 0.5).astype(int).flatten()
y_true = val_data.classes

# Generate classification report
report = classification_report(y_true, y_pred, target_names=["Non-Cancerous", "Cancerous"], output_dict=True)

# Convert to DataFrame for tabular display
report_df = pd.DataFrame(report).transpose()
print(report_df)

# Optional: Show nicely formatted table
import matplotlib.pyplot as plt
import seaborn as sns

plt.figure(figsize=(8, 4))
sns.heatmap(report_df.iloc[:-1, :-1], annot=True, cmap="Blues", fmt=".2f")
plt.title("Classification Report")
plt.show()

import matplotlib.pyplot as plt

# Plot training & validation accuracy values
plt.figure(figsize=(14, 5))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Train Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Train Loss', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', marker='o')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Create a figure with two subplots: one for accuracy, one for loss
plt.figure(figsize=(14, 5))

# ------------------------
# Accuracy Plot
# ------------------------
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='Training Accuracy', marker='o')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy', marker='o', linestyle='--')
plt.title('Training vs Validation Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.grid(True)

# ------------------------
# Loss Plot
# ------------------------
plt.subplot(1, 2, 2)
plt.plot(history.history['loss'], label='Training Loss', marker='o')
plt.plot(history.history['val_loss'], label='Validation Loss', marker='o', linestyle='--')
plt.title('Training vs Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)

# Display the plot
plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt

# Access training accuracy and loss from the history object
acc = history.history['accuracy']
loss = history.history['loss']
epochs = range(1, len(acc) + 1)

# Plot
plt.figure(figsize=(8, 6))
plt.plot(epochs, acc, 'bo-', label='Training Accuracy')
plt.plot(epochs, loss, 'ro-', label='Training Loss')
plt.title('Training Accuracy vs Training Loss')
plt.xlabel('Epochs')
plt.ylabel('Value')
plt.legend()
plt.grid(True)
plt.show()

# Print final training and validation accuracy
final_train_acc = history.history['accuracy'][-1]
final_val_acc = history.history['val_accuracy'][-1]

print(f"✅ Final Training Accuracy: {final_train_acc:.4f}")
print(f"✅ Final Validation Accuracy: {final_val_acc:.4f}")

# Evaluate model on validation set
val_loss, val_accuracy = model.evaluate(val_data)
print(f"✅ Model Accuracy on Validation Data: {val_accuracy:.4f}")



"""saved model and test file on google drive"""

!mkdir -p "/content/drive/MyDrive/ovarian_data"

!cp -r /content/train "/content/drive/MyDrive/ovarian_data/"
!cp -r /content/val "/content/drive/MyDrive/ovarian_data/"
!cp -r /content/test "/content/drive/MyDrive/ovarian_data/"

!mkdir -p "/content/drive/MyDrive/models"

!cp /content/best_model.h5 /content/drive/MyDrive/models/best_model.h5

!ls /content/drive/MyDrive/models/

"""**TESTING**"""

from tensorflow.keras.models import load_model

model = load_model('/content/drive/MyDrive/models/best_model.h5')

# Step 2: Define paths to your dataset folders inside Drive
train_dir = "/content/drive/MyDrive/ovarian_data/train"
val_dir   = "/content/drive/MyDrive/ovarian_data/val"
test_dir  = "/content/drive/MyDrive/ovarian_data/test"

# Step 3: Load data with augmentation (train) and rescaling (val/test)
from tensorflow.keras.preprocessing.image import ImageDataGenerator

#  Image preprocessing
train_datagen = ImageDataGenerator(
    rescale=1./255,
    rotation_range=10,
    zoom_range=0.2,
    horizontal_flip=True
)
val_datagen = ImageDataGenerator(rescale=1./255)

#  Create generators
train_data = train_datagen.flow_from_directory(
    train_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

val_data = val_datagen.flow_from_directory(
    val_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary'
)

test_data = val_datagen.flow_from_directory(
    test_dir,
    target_size=(224, 224),
    batch_size=32,
    class_mode='binary',
    shuffle=False  # Important for predictions later
)

# Print to confirm
print(" Data loaded successfully!")
print("Classes:", train_data.class_indices)







y_pred_probs = model.predict(test_data)
y_pred = (y_pred_probs > 0.5).astype("int32")

from tensorflow.keras.preprocessing.image import load_img, img_to_array

from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np
import matplotlib.pyplot as plt

path = "/content/test/Cancerous/10357_189590_aug_1.jpg"

# Load and preprocess image
img = load_img(path, target_size=(224, 224))
input_arr = img_to_array(img) / 255.0

# Show image
plt.imshow(input_arr)
plt.title("Input Image")
plt.axis("off")
plt.show()

# Expand dimensions for batch
input_arr = np.expand_dims(input_arr, axis=0)

# Predict
pred = model.predict(input_arr)
pred_class = (pred > 0.5).astype("int32")

# Print result
print(f"Predicted Class: {pred_class[0][0]} (Raw Prob: {pred[0][0]:.4f})")

if pred_class[0][0] == 0:
    print(" This image is **Cancerous**")
else:
    print("This image is **Non Cancerous**")

train_data.class_indices